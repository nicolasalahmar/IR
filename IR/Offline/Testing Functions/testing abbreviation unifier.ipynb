{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T19:32:56.339805Z",
     "start_time": "2024-04-28T19:32:56.334809Z"
    }
   },
   "cell_type": "code",
   "source": "text = \"Barack Obama was born in Hawaii, and he was the President of the United States and Kansas. (USA)\"",
   "id": "71d0a1e16c537259",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T19:32:57.071942Z",
     "start_time": "2024-04-28T19:32:57.051007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import pos_tag, ne_chunk, word_tokenize\n",
    "\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "ne_chunked_tokens = ne_chunk(tagged_tokens)\n",
    "print(ne_chunked_tokens)\n",
    "def extract_named_entities(chunked_sentence):\n",
    "  entities = []\n",
    "  for chunk in chunked_sentence:\n",
    "    if hasattr(chunk, 'label') and chunk.label() != 'O':\n",
    "      entities.append([x[0] for x in chunk.leaves()])\n",
    "  return entities\n",
    "\n",
    "entities = extract_named_entities(ne_chunked_tokens)\n",
    "print(entities)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  and/CC\n",
      "  (GPE Kansas/NNP)\n",
      "  ./.\n",
      "  (/(\n",
      "  (ORGANIZATION USA/NNP)\n",
      "  )/))\n",
      "[['Barack'], ['Obama'], ['Hawaii'], ['United', 'States'], ['Kansas'], ['USA']]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T19:05:29.362041Z",
     "start_time": "2024-04-28T19:05:27.467633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def expand_abbreviation_wordnet(abbr):\n",
    "  synsets = wordnet.synsets(abbr)\n",
    "  if synsets:\n",
    "    print(synsets)\n",
    "    synset = synsets[0]\n",
    "    lemma = synset.lemmas()[0]\n",
    "    return lemma.name().replace('_', ' ')\n",
    "  return abbr\n",
    "\n",
    "# for entity in entities:\n",
    "#   print(expand_abbreviation_wordnet(entity))\n",
    "print(expand_abbreviation_wordnet('United States of America'))\n"
   ],
   "id": "415782167ce1e279",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States of America\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T19:05:29.376393Z",
     "start_time": "2024-04-28T19:05:29.362041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "text = \"United States. United States of America \"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "#tokens = sent_tokenize(text)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "ne_chunked_tokens = ne_chunk(tagged_tokens)\n",
    "\n",
    "print(ne_chunked_tokens)"
   ],
   "id": "aeb9270be17ba176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  of/IN\n",
      "  (GPE America/NNP))\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T19:20:18.998097Z",
     "start_time": "2024-04-28T19:20:18.990583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def are_synonyms(word1):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word1):\n",
    "      synonyms = synonyms + [lemma.name().replace('_', ' ') for lemma in synset.lemmas()]\n",
    "      return synonyms\n",
    "\n",
    "print(are_synonyms(\"Syria\"))"
   ],
   "id": "b5b10190ffdaf0ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Syria', 'Syrian Arab Republic']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Spacey",
   "id": "fa833a0247b24540"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to generate entity mappings dynamically\n",
    "def generate_entity_mappings(doc):\n",
    "    entity_mappings = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and len(ent.text.split()) > 1:\n",
    "            # Take the first letter of each word in the organization name\n",
    "            abbreviation = ''.join(word[0] for word in ent.text.split())\n",
    "            entity_mappings[ent.text] = abbreviation\n",
    "    return entity_mappings\n",
    "\n",
    "# Function to map entity text to abbreviations\n",
    "def map_entity_to_abbreviation(entity_text, entity_mappings):\n",
    "    return entity_mappings.get(entity_text, entity_text)\n",
    "\n",
    "text = \"Apple is looking at buying World Health Organisation startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Generate entity mappings dynamically\n",
    "entity_mappings = generate_entity_mappings(doc)\n",
    "\n",
    "# Iterate over the entities in the document\n",
    "for ent in doc.ents:\n",
    "    # Get abbreviation if available, otherwise use the original text\n",
    "    abbreviated_entity = map_entity_to_abbreviation(ent.text, entity_mappings)\n",
    "    print(abbreviated_entity, ent.label_)"
   ],
   "id": "1170eeed9fd9094"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
